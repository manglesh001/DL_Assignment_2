{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 11415276,
          "sourceType": "datasetVersion",
          "datasetId": 7149398
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "DL_Ass_2_PartA_Q1",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manglesh001/DL_Assignment_2/blob/main/DL_Ass_2_PartA_Q1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "It_C-BClhiac"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "manglesh001_inaturalist12k_path = kagglehub.dataset_download('manglesh001/inaturalist12k')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Lkdn_RaBhiae"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import wandb\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-17T11:29:13.881124Z",
          "iopub.execute_input": "2025-04-17T11:29:13.881349Z",
          "iopub.status.idle": "2025-04-17T11:29:24.549356Z",
          "shell.execute_reply.started": "2025-04-17T11:29:13.88133Z",
          "shell.execute_reply": "2025-04-17T11:29:24.54876Z"
        },
        "id": "1Pk6p6ARhiaf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-17T11:29:24.550053Z",
          "iopub.execute_input": "2025-04-17T11:29:24.550503Z",
          "iopub.status.idle": "2025-04-17T11:29:24.560627Z",
          "shell.execute_reply.started": "2025-04-17T11:29:24.550466Z",
          "shell.execute_reply": "2025-04-17T11:29:24.559909Z"
        },
        "id": "8EpIDTU9hiaf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login 6001619563748a57b4114b0bb090fd4129ba6122"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-17T11:29:24.561919Z",
          "iopub.execute_input": "2025-04-17T11:29:24.562115Z",
          "iopub.status.idle": "2025-04-17T11:29:25.918063Z",
          "shell.execute_reply.started": "2025-04-17T11:29:24.5621Z",
          "shell.execute_reply": "2025-04-17T11:29:25.917105Z"
        },
        "id": "zmC6N-uThiag",
        "outputId": "8cd70d02-a900-4b9d-8ea6-6b4ecab75909"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1-** Build a small CNN model consisting of 5 convolution layers. Each convolution layer would be followed by an activation and a max-pooling layer.\n",
        "\n",
        "After 5 such conv-activation-maxpool blocks, you should have one dense layer followed by the output layer containing 10 neurons (1 for each of the 10 classes)."
      ],
      "metadata": {
        "id": "gSrVO6z4hiai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=10, filter_counts=[32, 64, 128, 256, 512], filter_sizes=[(3, 3), (3, 3), (3, 3), (3, 3), (3, 3)],\n",
        "                 activation_func=nn.ReLU(), fc_neurons=512, use_batch_norm=True,dropout_rate=0.2):\n",
        "        super(CNNModel, self).__init__()\n",
        "        in_channels = 3  #RGB channels\n",
        "        # Store layers in List\n",
        "        self.features = nn.ModuleList()\n",
        "        # doing 5 conv-act-maxpool blocks\n",
        "        for i in range(5):\n",
        "            # Add  2D convolution layer\n",
        "            conv = nn.Conv2d(in_channels, filter_counts[i], kernel_size=filter_sizes[i], padding='same')\n",
        "            self.features.append(conv)\n",
        "            # Add batch normalization\n",
        "            if use_batch_norm:\n",
        "                self.features.append(nn.BatchNorm2d(filter_counts[i]))\n",
        "            # Add activation function\n",
        "            self.features.append(activation_func)\n",
        "            # Add max pooling layer\n",
        "            self.features.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "            # Update in_channels for the next layer\n",
        "            in_channels = filter_counts[i]\n",
        "        # Adaptive pooling to handle different input sizes\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        # Fully connected layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(filter_counts[-1], fc_neurons),activation_func,\n",
        "            nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity(),\n",
        "            nn.Linear(fc_neurons, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass input through feature layers\n",
        "        for layer in self.features:\n",
        "            x = layer(x)\n",
        "        # Global average pooling\n",
        "        x = self.adaptive_pool(x)\n",
        "        # Flatten the tensor\n",
        "        x = torch.flatten(x, 1)\n",
        "        # Pass through classifier\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-17T11:29:25.919158Z",
          "iopub.execute_input": "2025-04-17T11:29:25.91941Z",
          "iopub.status.idle": "2025-04-17T11:29:25.929908Z",
          "shell.execute_reply.started": "2025-04-17T11:29:25.919388Z",
          "shell.execute_reply": "2025-04-17T11:29:25.929301Z"
        },
        "id": "ycI_PPa1hiaj"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}